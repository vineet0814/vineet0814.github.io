---
title: What is Direct Preference Optimization
tags: [LLM, Fine Tuning]
style: 
color: secondary
description: Reinforcement Learning with Human Feedback is the current state-of-the-art technique to fine tune LLMs. However, a recent and a much simpler  improvement on RLHF was published in paper titled 'Direct Preference Optimization-Your Language Model is Secretly a Reward Model'. 
external_url: https://buttered-jupiter-2a0.notion.site/LLM-Fine-Tuning-6bbd7d58e4ab402dbccc077d84b4efc1?pvs=4
---
